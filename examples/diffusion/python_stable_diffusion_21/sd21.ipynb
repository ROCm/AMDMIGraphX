{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The MIT License (MIT)\n",
    "#\n",
    "#  Copyright (c) 2015-2024 Advanced Micro Devices, Inc. All rights reserved.\n",
    "#\n",
    "#  Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "#  of this software and associated documentation files (the 'Software'), to deal\n",
    "#  in the Software without restriction, including without limitation the rights\n",
    "#  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "#  copies of the Software, and to permit persons to whom the Software is\n",
    "#  furnished to do so, subject to the following conditions:\n",
    "#\n",
    "#  The above copyright notice and this permission notice shall be included in\n",
    "#  all copies or substantial portions of the Software.\n",
    "#\n",
    "#  THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\n",
    "#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "#  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "#  THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion 2.1\n",
    "\n",
    "The following example will show how to run `Stable Diffusion 2.1` with `MIGraphX`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# We need this version to run torch with gpu tensors\n",
    "!pip install torch==2.1.1 -f https://repo.radeon.com/rocm/manylinux/rocm-rel-6.0/\n",
    "!pip install optimum[onnxruntime] transformers diffusers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use optimum to generate the onnx files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export models\n",
    "!optimum-cli export onnx --model stabilityai/stable-diffusion-2-1 models/sd21-onnx --task stable-diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use torch tensors for all calculation. Everything will be allocated on the GPU to avoid Host-Device copies.\n",
    "\n",
    "We installed the rocm version of pytorch, let's confirm that we can access the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.version\n",
    "\n",
    "print(f\"{torch.cuda.is_available() = }\")\n",
    "print(f\"{torch.cuda.get_device_name(0) = }\")\n",
    "print(f\"{torch.version.cuda = }\")\n",
    "print(f\"{torch.version.hip = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is not working properly, try restaring the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to load these models with python.\n",
    "\n",
    "First, we make sure that MIGraphX module is found in the python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "mgx_lib_path = \"/opt/rocm/lib/\"\n",
    "# or if you locally built MIGraphX\n",
    "# mgx_lib_path = \"/code/AMDMIGraphX/build/lib/\"\n",
    "if mgx_lib_path not in sys.path:\n",
    "    sys.path.append(mgx_lib_path)\n",
    "import migraphx as mgx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a helper method to load and cache the models.\n",
    "\n",
    "This will use the `models/sd21-onnx` path. If you changed it, make sure to update here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# helper for model loading\n",
    "def load_mgx_model(name, shapes):\n",
    "    file = f\"models/sd21-onnx/{name}/model\"\n",
    "    print(f\"Loading {name} model from {file}\")\n",
    "    if os.path.isfile(f\"{file}.mxr\"):\n",
    "        print(f\"Found mxr, loading it...\")\n",
    "        model = mgx.load(f\"{file}.mxr\", format=\"msgpack\")\n",
    "    elif os.path.isfile(f\"{file}.onnx\"):\n",
    "        print(f\"Parsing from onnx file...\")\n",
    "        model = mgx.parse_onnx(f\"{file}.onnx\", map_input_dims=shapes)\n",
    "        model.compile(mgx.get_target(\"gpu\"), offload_copy=False)\n",
    "        print(f\"Saving {name} model to mxr file...\")\n",
    "        mgx.save(model, f\"{file}.mxr\", format=\"msgpack\")\n",
    "    else:\n",
    "        print(f\"No {name} model found. Please verify the path is correct and re-try, or re-download model.\")\n",
    "        sys.exit(1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can load the models. This could take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = load_mgx_model(\"text_encoder\", {\"input_ids\": [2, 77]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = load_mgx_model(\n",
    "        \"unet\", {\n",
    "            \"sample\": [2, 4, 64, 64],\n",
    "            \"encoder_hidden_states\": [2, 77, 1024],\n",
    "            \"timestep\": [1],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = load_mgx_model(\"vae_decoder\", {\"latent_sample\": [1, 4, 64, 64]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass a tensor to MIGraphX, first we need to convert it an argument.\n",
    "\n",
    "We avoid the copy via allocating the tensor on the gpu, so we only need to pass the address of the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to have a mapping between torch and migraphx data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgx_to_torch_dtype_dict = {\n",
    "    \"bool_type\": torch.bool,\n",
    "    \"uint8_type\": torch.uint8,\n",
    "    \"int8_type\": torch.int8,\n",
    "    \"int16_type\": torch.int16,\n",
    "    \"int32_type\": torch.int32,\n",
    "    \"int64_type\": torch.int64,\n",
    "    \"float_type\": torch.float32,\n",
    "    \"double_type\": torch.float64,\n",
    "    \"half_type\": torch.float16,\n",
    "}\n",
    "\n",
    "torch_to_mgx_dtype_dict = {\n",
    "    value: key\n",
    "    for (key, value) in mgx_to_torch_dtype_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a way to allocate the torch buffers for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_torch_tensors(model):\n",
    "    input_shapes = model.get_parameter_shapes()\n",
    "    data_mapping = {\n",
    "        name:\n",
    "        torch.zeros(shape.lens()).to(\n",
    "            mgx_to_torch_dtype_dict[shape.type_string()]).to(device=\"cuda\")\n",
    "        for name, shape in input_shapes.items()\n",
    "    }\n",
    "    return data_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we allocate tensors for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_tensors = allocate_torch_tensors(text_encoder)\n",
    "unet_tensors = allocate_torch_tensors(unet)\n",
    "vae_tensors = allocate_torch_tensors(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to tell MIGraphX how to access these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_arg(tensor):\n",
    "    return mgx.argument_from_pointer(\n",
    "        mgx.shape(\n",
    "            **{\n",
    "                \"type\": torch_to_mgx_dtype_dict[tensor.dtype],\n",
    "                \"lens\": list(tensor.size()),\n",
    "                \"strides\": list(tensor.stride())\n",
    "            }), tensor.data_ptr())\n",
    "\n",
    "def tensors_to_args(tensors):\n",
    "    return {name: tensor_to_arg(tensor) for name, tensor in tensors.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tensors won't change, we only need to do this once, an cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_args = tensors_to_args(text_encoder_tensors)\n",
    "unet_args = tensors_to_args(unet_tensors)\n",
    "vae_args = tensors_to_args(vae_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs will be called `main:#output_*`. We create a helper to access them more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_name(idx):\n",
    "    return f\"main:#output_{idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the remaining packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import EulerDiscreteScheduler\n",
    "from transformers import CLIPTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to load the scheduler and tokenizer from the original source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id,\n",
    "                                                   subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define all the steps one by one, to make the last step short and simple.\n",
    "\n",
    "The first step will be to tokenize the user prompt. It will make a `(1, 77)` shaped `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(*inputs):\n",
    "    return tokenizer([*inputs],\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=tokenizer.model_max_length,\n",
    "                     truncation=True,\n",
    "                     return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "test_tk = tokenize(\"test tokenizer to see the tokens\")\n",
    "test_tk.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the tokenized prompts through the `Text Encoder` model. It expects the `(2, 77)` data as `int32`. It is `2` because we will also pass the negative prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "text_encoder.get_parameter_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(prompt_tokens):\n",
    "    text_encoder_tensors[\"input_ids\"].copy_(prompt_tokens.input_ids.to(torch.int32))\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    text_encoder.run(text_encoder_args)\n",
    "    mgx.gpu_sync()\n",
    "\n",
    "    return text_encoder_tensors[get_output_name(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "test_emb = get_embeddings(tokenize(\"test tokenizer to see the tokens\"))\n",
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other input of the model is latent representation (pure noise). It will be transformed into a 512x512 image later.\n",
    "The last input will be the timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latents(seed):\n",
    "    return torch.randn(\n",
    "        (1, 4, 64, 64),\n",
    "        generator=torch.manual_seed(seed),\n",
    "    ).to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "test_latents = generate_latents(42)\n",
    "test_latents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add two helpers to access and convert from torch to numpy with the proper datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_sample(latents, t):\n",
    "    return scheduler.scale_model_input(latents, t).to(torch.float32).to(device=\"cuda\")\n",
    "\n",
    "def get_timestep(t):\n",
    "    return torch.atleast_1d(t.to(torch.int64).to(device=\"cuda\"))  # convert 0D -> 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNet model will be run in a loop. It will predict the noise residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "unet.get_parameter_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(sample, embeddings, timestep):\n",
    "    unet_tensors[\"sample\"].copy_(sample)\n",
    "    unet_tensors[\"encoder_hidden_states\"].copy_(embeddings)\n",
    "    unet_tensors[\"timestep\"].copy_(timestep)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    unet.run(unet_args)\n",
    "    mgx.gpu_sync()\n",
    "\n",
    "    return torch.tensor_split(unet_tensors[get_output_name(0)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers to do the classifier-free guidance and computing the previous noisy sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_guidance(noise_pred_uncond, noise_pred_text, scale):\n",
    "    return noise_pred_uncond + scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "def compute_previous(noise_pred, t, latents):\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    return scheduler.step(noise_pred, t, latents).prev_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale and decode the image latents with VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_denoised(latents):\n",
    "    return 1 / 0.18215 * latents\n",
    "\n",
    "\n",
    "def decode(latents):\n",
    "    vae_tensors[\"latent_sample\"].copy_(latents)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    vae.run(vae_args)\n",
    "    mgx.gpu_sync()\n",
    "\n",
    "    return vae_tensors[get_output_name(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, we need to convert it to an image to display or save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_rgb_image(image):\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    return Image.fromarray(images[0])\n",
    "\n",
    "def save_image(pil_image, filename=\"output.png\"):\n",
    "    pil_image.save(filename, format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with these params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse\"\n",
    "negative_prompt = \"\"\n",
    "steps = 20\n",
    "seed = 13\n",
    "scale = 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, to put everything together and run the whole pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(steps, device=\"cuda\")\n",
    "\n",
    "input_tokens = tokenize(prompt, negative_prompt)\n",
    "text_embeddings = get_embeddings(input_tokens)\n",
    "latents = generate_latents(seed) * scheduler.init_noise_sigma\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    sample = get_scaled_sample(torch.cat([latents] * 2), t)\n",
    "    timestep = get_timestep(t)\n",
    "\n",
    "    noise_pred_text, noise_pred_uncond = denoise(sample, text_embeddings, timestep)\n",
    "\n",
    "    noise_pred = perform_guidance(noise_pred_uncond, noise_pred_text, scale)\n",
    "    latents = compute_previous(noise_pred, t, latents)\n",
    "\n",
    "latents = scale_denoised(latents)\n",
    "result = decode(latents)\n",
    "image = convert_to_rgb_image(result)\n",
    "\n",
    "# show the image\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like the generated image, save it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(image, \"output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
