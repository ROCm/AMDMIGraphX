{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFNet Inference with AMD MIGraphX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer-Free ResNet is a new residual convolutional network providing new state-of-the-art Top-1 accuracy of 86.5% at ImageNet dataset. The most important feature of the model is removing batch normalization. Instead of batch normalization, it uses adaptive gradient clipping, to provide same regularization effect of BatchNorm. <br> Details of this network: https://arxiv.org/abs/2102.06171\n",
    "\n",
    "In this notebook, we are showing: <br>\n",
    "- How to convert NFNet PyTorch model to ONNX.\n",
    "- How to optimize NFNet ONNX model with AMD MIGraphX.\n",
    "- How to run inference on AMD GPU with the optimized ONNX model.\n",
    "\n",
    "The NFNet utilized in this example is the smallest NFNet version, F0: 71.5M parameters (83.6% top-1 accuracy on ImageNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Install python3-opencv package first as it provides libraries required for cv2 package.<br>It is recommended you install it through terminal, as it asks for additional inputs.\n",
    "\n",
    "`apt-get update & apt-get install python3-opencv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.0.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from -r requirements_nfnet.txt (line 1)) (4.5.1.48)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from -r requirements_nfnet.txt (line 2)) (0.8)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.6/dist-packages (from -r requirements_nfnet.txt (line 3)) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime->-r requirements_nfnet.txt (line 3)) (1.18.5)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime->-r requirements_nfnet.txt (line 3)) (3.15.6)\n",
      "Requirement already satisfied: six>=1.9 in /usr/lib/python3/dist-packages (from protobuf->onnxruntime->-r requirements_nfnet.txt (line 3)) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip #needed for opencv-python installation\n",
    "!pip3 install -r requirements_nfnet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "import time\n",
    "from os import path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing AMD MIGraphX Python Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import migraphx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NFNet ONNX file\n",
    "Following repository provides functionality to create NFNet ONNX file from PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch-image-models' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: clean-up this flow.\n",
    "!git clone https://github.com/cagery/pytorch-image-models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r ./pytorch-image-models/requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from -r ./pytorch-image-models/requirements.txt (line 2)) (0.9.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from -r ./pytorch-image-models/requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r ./pytorch-image-models/requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r ./pytorch-image-models/requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r ./pytorch-image-models/requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.5.0->-r ./pytorch-image-models/requirements.txt (line 2)) (8.2.0)\n",
      "Traceback (most recent call last):\n",
      "  File \"./pytorch-image-models/pt_to_onnx.py\", line 51, in <module>\n",
      "    testOnnxFile()\n",
      "  File \"./pytorch-image-models/pt_to_onnx.py\", line 47, in testOnnxFile\n",
      "    onnx_model = onnx.load(\"../dm_nfnet_f0.onnx\")\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/onnx/__init__.py\", line 114, in load_model\n",
      "    s = _load_bytes(f)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/onnx/__init__.py\", line 30, in _load_bytes\n",
      "    with open(cast(Text, f), 'rb') as readable:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../dm_nfnet_f0.onnx'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ./pytorch-image-models/requirements.txt\n",
    "!python3 ./pytorch-image-models/pt_to_onnx.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ImageNet labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../python_api_inference/imagenet_simple_labels.json') as json_data:\n",
    "    labels = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load ONNX model using MIGraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = migraphx.parse_onnx(\"dm_nfnet_f0.onnx\")\n",
    "model.compile(migraphx.get_target(\"gpu\"))\n",
    "\n",
    "print(model.get_parameter_names())\n",
    "print(model.get_parameter_shapes())\n",
    "print(model.get_output_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nxn(image, n):\n",
    "    height, width = image.shape[:2]    \n",
    "    if height > width:\n",
    "        dif = height - width\n",
    "        bar = dif // 2 \n",
    "        square = image[(bar + (dif % 2)):(height - bar),:]\n",
    "        return cv2.resize(square, (n, n))\n",
    "    elif width > height:\n",
    "        dif = width - height\n",
    "        bar = dif // 2\n",
    "        square = image[:,(bar + (dif % 2)):(width - bar)]\n",
    "        return cv2.resize(square, (n, n))\n",
    "    else:\n",
    "        return cv2.resize(image, (n, n))\n",
    "    \n",
    "def preprocess(img_data):\n",
    "    mean_vec = np.array([0.485, 0.456, 0.406])\n",
    "    stddev_vec = np.array([0.229, 0.224, 0.225])\n",
    "    norm_img_data = np.zeros(img_data.shape).astype('float32')\n",
    "    for i in range(img_data.shape[0]):  \n",
    "        norm_img_data[i,:,:] = (img_data[i,:,:]/255 - mean_vec[i]) / stddev_vec[i]\n",
    "    return norm_img_data\n",
    "\n",
    "def input_process(frame, dim):\n",
    "    # Crop and resize original image\n",
    "    cropped = make_nxn(frame, dim)\n",
    "    # Convert from HWC to CHW\n",
    "    chw = cropped.transpose(2,0,1)\n",
    "    # Apply normalization\n",
    "    pp = preprocess(chw)\n",
    "    # Add singleton dimension (CHW to NCHW)\n",
    "    data = np.expand_dims(pp.astype('float32'),0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-22 22:26:52--  http://farm5.static.flickr.com/4072/4462811418_8bc2bd42ca_z_d.jpg\n",
      "Resolving farm5.static.flickr.com (farm5.static.flickr.com)... 143.204.162.96, 2600:9000:20ef:2000:0:5a51:64c9:c681, 2600:9000:20ef:2200:0:5a51:64c9:c681, ...\n",
      "Connecting to farm5.static.flickr.com (farm5.static.flickr.com)|143.204.162.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://farm5.static.flickr.com/4072/4462811418_8bc2bd42ca_z_d.jpg [following]\n",
      "--2021-04-22 22:26:52--  https://farm5.static.flickr.com/4072/4462811418_8bc2bd42ca_z_d.jpg\n",
      "Connecting to farm5.static.flickr.com (farm5.static.flickr.com)|143.204.162.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [image/jpeg]\n",
      "Saving to: ‘traffic_light.jpg’\n",
      "\n",
      "traffic_light.jpg       [ <=>                ]  52.04K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-04-22 22:26:52 (2.23 MB/s) - ‘traffic_light.jpg’ saved [53289]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch example image\n",
    "!wget http://farm5.static.flickr.com/4072/4462811418_8bc2bd42ca_z_d.jpg -O traffic_light.jpg\n",
    "# Read the image\n",
    "im = cv2.imread('traffic_light.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time inference took: 2.75ms\n",
      "\n",
      "Result: traffic light\n"
     ]
    }
   ],
   "source": [
    "# Process the read image to conform input requirements\n",
    "data_input = input_process(im, 192)\n",
    "\n",
    "# Run the model\n",
    "start = time.time()\n",
    "results = model.run({'inputs':data_input}) # Your first inference would take longer than the following ones.\n",
    "print(f\"Time inference took: {100*(time.time() - start):.2f}ms\")\n",
    "# Extract the index of the top prediction\n",
    "res_npa = np.array(results[0])\n",
    "print(f\"\\nResult: {labels[np.argmax(res_npa)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
