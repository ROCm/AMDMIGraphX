{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-SQuAD Inference Example with AMD MIGraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to run the BERT-Squad model on ONNX-Runtime with MIGraphX backend.<br>\n",
    "**Steps:**<br>**1)** Make sure you are using the Dockerfile at https://github.com/ROCmSoftwarePlatform/AMDMIGraphX/blob/develop/Dockerfile , and build and run the container.<br>\n",
    "**2)** Run Jupyter server and use this notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/lib/python36.zip',\n",
       " '/usr/lib/python3.6',\n",
       " '/usr/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/usr/local/lib/python3.6/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/opt/rocm/lib',\n",
       " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
       " '/root/.ipython']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sample:\n",
    "Write the input file that includes the context paragraph and the questions for the model to answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputs.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputs.json\n",
    "{\n",
    "  \"version\": \"1.4\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"paragraphs\": [\n",
    "        {\n",
    "          \"context\": \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\",\n",
    "          \"qas\": [\n",
    "            {\n",
    "              \"question\": \"where is the businesses choosing to go?\",\n",
    "              \"id\": \"1\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"how may votes did the ballot measure need?\",\n",
    "              \"id\": \"2\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"By what year many Silicon Valley businesses were choosing the Moscone Center?\",\n",
    "              \"id\": \"3\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"title\": \"Conference Center\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ONNX-Runtime backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1f7b0690d333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_available_providers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort.get_available_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Download the uncased file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract parameters from the given input and convert it into features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow==1.15\n",
    "import numpy as np\n",
    "import tokenization\n",
    "import os\n",
    "from run_onnx_squad import *\n",
    "import json\n",
    "\n",
    "input_file = 'inputs.json'\n",
    "with open(input_file) as json_file:  \n",
    "    test_data = json.load(json_file)\n",
    "    print(json.dumps(test_data, indent=2))\n",
    "  \n",
    "# preprocess input\n",
    "predict_file = 'inputs.json'\n",
    "\n",
    "# Use read_squad_examples method from run_onnx_squad to read the input file\n",
    "eval_examples = read_squad_examples(input_file=predict_file)\n",
    "\n",
    "max_seq_length = 256\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "batch_size = 1\n",
    "n_best_size = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "\n",
    "vocab_file = os.path.join('uncased_L-12_H-768_A-12', 'vocab.txt')\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
    "\n",
    "my_list = []\n",
    "\n",
    "\n",
    "# Use convert_examples_to_features method from run_onnx_squad to get parameters from the input \n",
    "input_ids, input_mask, segment_ids, extra_data = convert_examples_to_features(eval_examples, tokenizer, \n",
    "                                                                              max_seq_length, doc_stride, max_query_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Run the ONNX model under onnxruntime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an onnx inference session and run the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/onnx/models/text/master/vision/classification/resnet/model/resnet50-v2-7.onnx?raw=true\n",
    "!wget https://github.com/onnx/models/raw/master/text/machine_comprehension/bert-squad/model/bertsquad-10.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "\n",
    "session = ort.InferenceSession('bertsquad-10.onnx')\n",
    "\n",
    "for input_meta in session.get_inputs():\n",
    "    print(input_meta)\n",
    "n = len(input_ids)\n",
    "bs = batch_size\n",
    "all_results = []\n",
    "start = timer()\n",
    "for idx in range(0, n):\n",
    "    item = eval_examples[idx]\n",
    "    # this is using batch_size=1\n",
    "    # feed the input data as int64\n",
    "    data = {\"unique_ids_raw_output___9:0\": np.array([item.qas_id], dtype=np.int64),\n",
    "            \"input_ids:0\": input_ids[idx:idx+bs],\n",
    "            \"input_mask:0\": input_mask[idx:idx+bs],\n",
    "            \"segment_ids:0\": segment_ids[idx:idx+bs]}\n",
    "    result = session.run([\"unique_ids:0\",\"unstack:0\", \"unstack:1\"], data)\n",
    "    in_batch = result[1].shape[0]\n",
    "    start_logits = [float(x) for x in result[1][0].flat]\n",
    "    end_logits = [float(x) for x in result[2][0].flat]\n",
    "    for i in range(0, in_batch):\n",
    "        unique_id = len(all_results)\n",
    "        all_results.append(RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Postprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the predictions (answers to the input questions) in a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing\n",
    "output_dir = 'predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
    "write_predictions(eval_examples, extra_data, all_results,\n",
    "                  n_best_size, max_answer_length,\n",
    "                  True, output_prediction_file, output_nbest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "import json\n",
    "with open(output_prediction_file) as json_file:  \n",
    "    test_data = json.load(json_file)\n",
    "    print(json.dumps(test_data, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
